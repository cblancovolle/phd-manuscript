@inproceedings{10.1007/978-3-031-77367-9_26,
  title = {Explainability and Interpretability of an Ensemble Multi-Agent System for Supervised Learning},
  booktitle = {{{PRIMA}} 2024: {{Principles}} and Practice of Multi-Agent Systems},
  author = {{Blanco-Volle}, Cl{\'e}ment and Verstaevel, Nicolas and Combettes, St{\'e}phanie and Gleizes, Marie-Pierre and Povlovitsch Seixas, Michel},
  editor = {Arisaka, Ryuta and {Sanchez-Anguix}, Victor and Stein, Sebastian and Aydo{\u g}an, Reyhan and {van der Torre}, Leon and Ito, Takayuki},
  year = 2025,
  pages = {335--350},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  abstract = {We present a multi-agent ensemble learning approach for supervised learning and evaluate its performance on a task of nonlinear function approximation. This approach relies on a set of learning agents that self-organize according to cooperation rules. We study the properties of this type of system in terms of explainability and interpretability of the prediction process by analyzing the shapes of the agents and their spatial organization. A comparative study on synthetic datasets generated from nonlinear 2D functions is also conducted to evaluate the performance. The results indicate that our multi-agent approach achieves prediction scores similar to state-of-the-art approaches and introduces new properties contributing to address the problematic of explainability in supervised learning.},
  isbn = {978-3-031-77367-9}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = 2018,
  month = sep,
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  publisher = {IEEE},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  langid = {english}
}

@article{aliNumericalEvaluationSeveral2005,
  title = {A {{Numerical Evaluation}} of {{Several Stochastic Algorithms}} on {{Selected Continuous Global Optimization Test Problems}}},
  author = {Ali, M. Montaz and Khompatraporn, Charoenchai and Zabinsky, Zelda B.},
  year = 2005,
  month = apr,
  journal = {J. Global Optim.},
  volume = {31},
  number = {4},
  pages = {635--672},
  publisher = {Kluwer Academic Publishers},
  issn = {1573-2916},
  doi = {10.1007/s10898-004-9972-2},
  abstract = {There is a need for a methodology to fairly compare and present evaluation study results of stochastic global optimization algorithms. This need raises two important questions of (i) an appropriate set of benchmark test problems that the algorithms may be tested upon and (ii) a methodology to compactly and completely present the results. To address the first question, we compiled a collection of test problems, some are better known than others. Although the compilation is not exhaustive, it provides an easily accessible collection of standard test problems for continuous global optimization. Five different stochastic global optimization algorithms have been tested on these problems and a performance profile plot based on the improvement of objective function values is constructed to investigate the macroscopic behavior of the algorithms. The paper also investigates the microscopic behavior of the algorithms through quartile sequential plots, and contrasts the information gained from these two kinds of plots. The effect of the length of run is explored by using three maximum numbers of function evaluations and it is shown to significantly impact the behavior of the algorithms.},
  keywords = {Computer Science,general,Operations Research/Decision Theory,Optimization,Real Functions}
}

@article{bauer2016understanding,
  title = {Understanding Probabilistic Sparse {{Gaussian}} Process Approximations},
  author = {Bauer, Matthias and {Van der Wilk}, Mark and Rasmussen, Carl Edward},
  year = 2016,
  journal = {Advances in neural information processing systems},
  volume = {29}
}

@article{berberich2025overview,
  title = {An Overview of Systems-Theoretic Guarantees in Data-Driven Model Predictive Control},
  author = {Berberich, Julian and Allg{\"o}wer, Frank},
  year = 2025,
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {8},
  number = {1},
  pages = {77--100},
  publisher = {Annual Reviews}
}

@inproceedings{berkenkamp2015safe,
  title = {Safe and Robust Learning Control with {{Gaussian}} Processes},
  booktitle = {2015 European Control Conference ({{ECC}})},
  author = {Berkenkamp, Felix and Schoellig, Angela P},
  year = 2015,
  pages = {2496--2501},
  publisher = {IEEE}
}

@inproceedings{bifet2009adaptive,
  title = {Adaptive Learning from Evolving Data Streams},
  booktitle = {International Symposium on Intelligent Data Analysis},
  author = {Bifet, Albert and Gavalda, Ricard},
  year = 2009,
  pages = {249--260},
  publisher = {Springer}
}

@inproceedings{blanco2024explainability,
  title = {Explainability and Interpretability of an Ensemble Multi-Agent System for Supervised Learning},
  booktitle = {International Conference on Principles and Practice of Multi-Agent Systems},
  author = {{Blanco-Volle}, Cl{\'e}ment and Verstaevel, Nicolas and Combettes, St{\'e}phanie and Gleizes, Marie-Pierre and Povlovitsch Seixas, Michel},
  year = 2024,
  pages = {335--350},
  publisher = {Springer}
}

@incollection{boesSelfAdaptiveContextLearning2015,
  title = {The {{Self-Adaptive Context Learning Pattern}}: {{Overview}} and {{Proposal}}},
  booktitle = {{{SpringerLink}}},
  author = {Boes, J{\'e}r{\'e}my and Nigon, Julien and Verstaevel, Nicolas and Gleizes, Marie-Pierre and Migeon, Fr{\'e}d{\'e}ric},
  year = 2015,
  month = dec,
  pages = {91--104},
  publisher = {Springer},
  address = {Cham, Switzerland},
  doi = {10.1007/978-3-319-25591-0_7},
  abstract = {Over the years, our research group has designed and developed many self-adaptive multi-agent systems to tackle real-world complex problems, such as robot control and heat engine optimization. A recurrent key feature of these systems is the ability to learn how to...},
  isbn = {1611-3349, 978-3-319-25591-0},
  langid = {english},
  keywords = {Adaptation,Context,Cooperation,Learning,Machine learning,Multi-agent system,Self-organisation,ToRead},
  file = {C:\Users\uif72473\Zotero\storage\838DSUC4\Boes et al. - 2015 - The Self-Adaptive Context Learning Pattern Overvi.pdf}
}

@article{boggs1995sequential,
  title = {Sequential Quadratic Programming},
  author = {Boggs, Paul T and Tolle, Jon W},
  year = 1995,
  journal = {Acta numerica},
  volume = {4},
  pages = {1--51},
  publisher = {Cambridge University Press}
}

@article{breiman1996bagging,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = 1996,
  journal = {Machine learning},
  volume = {24},
  number = {2},
  pages = {123--140},
  publisher = {Springer}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification and {{Regression Trees}}},
  author = {Breiman, Friedman},
  year = 2017,
  month = oct,
  journal = {Taylor \& Francis},
  publisher = {Taylor \& Francis},
  address = {Andover, England, UK},
  doi = {10.1201/9781315139470},
  abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and},
  isbn = {978-1-315-13947-0},
  langid = {english}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = 2001,
  month = oct,
  journal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  publisher = {Kluwer Academic Publishers},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  keywords = {Artificial Intelligence,Control,Machine Learning,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling}
}

@article{Brunke2022May,
  title = {Safe Learning in Robotics: {{From}} Learning-Based Control to Safe Reinforcement Learning},
  author = {Brunke, Lukas and Greeff, Melissa and Hall, Adam W. and Yuan, Zhaocong and Zhou, Siqi and Panerati, Jacopo and Schoellig, Angela P.},
  year = 2022,
  month = may,
  journal = {Annu. Rev. Control Rob. Auton. Syst.},
  number = {Volume 5, 2022},
  pages = {411--444},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-control-042920-020211}
}

@article{burkart2021survey,
  title = {A Survey on the Explainability of Supervised Machine Learning},
  author = {Burkart, Nadia and Huber, Marco F},
  year = 2021,
  journal = {Journal of Artificial Intelligence Research},
  volume = {70},
  pages = {245--317}
}

@article{canese2021multi,
  title = {Multi-Agent Reinforcement Learning: {{A}} Review of Challenges and Applications},
  author = {Canese, Lorenzo and Cardarilli, Gian Carlo and Di Nunzio, Luca and Fazzolari, Rocco and Giardino, Daniele and Re, Marco and Span{\`o}, Sergio},
  year = 2021,
  journal = {Applied Sciences},
  volume = {11},
  number = {11},
  pages = {4948},
  publisher = {MDPI},
  keywords = {ToRead},
  file = {C:\Users\uif72473\Zotero\storage\B6Y4XPVK\Canese et al. - 2021 - Multi-agent reinforcement learning A review of ch.pdf}
}

@article{changLIBSVMLibrarySupport2011,
  title = {{{LIBSVM}}: {{A}} Library for Support Vector Machines},
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  year = 2011,
  month = may,
  journal = {ACM Trans. Intell. Syst. Technol.},
  volume = {2},
  number = {3},
  pages = {1--27},
  publisher = {Association for Computing Machinery},
  issn = {2157-6904},
  doi = {10.1145/1961189.1961199},
  abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
  keywords = {Classification LIBSVM optimization regression support vector machines SVM}
}

@article{cheng2016incremental,
  title = {Incremental Variational Sparse {{Gaussian}} Process Regression},
  author = {Cheng, Ching-An and Boots, Byron},
  year = 2016,
  journal = {Advances in Neural Information Processing Systems},
  volume = {29}
}

@article{chenXGBoostScalableTree2016,
  title = {{{XGBoost}}: {{A Scalable Tree Boosting System}}},
  author = {Chen, Tianqi and Guestrin, Carlos},
  year = 2016,
  month = mar,
  journal = {arXiv},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  langid = {english},
  keywords = {Machine Learning (cs.LG)}
}

@article{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = 2018,
  month = may,
  journal = {arXiv},
  doi = {10.48550/arXiv.1805.12114},
  abstract = {Model-based reinforcement learning (RL) algorithms can attain excellent sample efficiency, but often lag behind the best model-free algorithms in terms of asymptotic performance. This is especially true with high-capacity parametric function approximators, such as deep networks. In this paper, we study how to bridge this gap, by employing uncertainty-aware dynamics models. We propose a new algorithm called probabilistic ensembles with trajectory sampling (PETS) that combines uncertainty-aware deep network dynamics models with sampling-based uncertainty propagation. Our comparison to state-of-the-art model-based and model-free deep RL algorithms shows that our approach matches the asymptotic performance of model-free algorithms on several challenging benchmark tasks, while requiring significantly fewer samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and Proximal Policy Optimization respectively on the half-cheetah task).},
  langid = {english},
  keywords = {Artificial Intelligence (cs.AI),Machine Learning (cs.LG),Machine Learning (stat.ML),Robotics (cs.RO)}
}

@phdthesis{dato2021apprentissage,
  title = {Apprentissage Permanent Par Feedback Endog\`ene, Application \`a Un Syst\`eme Robotique},
  author = {Dato, Bruno},
  year = 2021,
  school = {Toulouse 3},
  file = {C:\Users\uif72473\Zotero\storage\499H5V38\Dato - 2021 - Apprentissage permanent par feedback endogène, app.pdf}
}

@article{de2005tutorial,
  title = {A Tutorial on the Cross-Entropy Method},
  author = {De Boer, Pieter-Tjerk and Kroese, Dirk P and Mannor, Shie and Rubinstein, Reuven Y},
  year = 2005,
  journal = {Annals of operations research},
  volume = {134},
  number = {1},
  pages = {19--67},
  publisher = {Springer}
}

@incollection{dietterichEnsembleMethodsMachine2000,
  title = {Ensemble {{Methods}} in {{Machine Learning}}},
  booktitle = {{{SpringerLink}}},
  author = {Dietterich, Thomas G.},
  year = 2000,
  month = dec,
  pages = {1--15},
  publisher = {Springer},
  address = {Berlin, Germany},
  doi = {10.1007/3-540-45014-9_1},
  abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting...},
  isbn = {978-3-540-45014-6},
  langid = {english},
  keywords = {Decision Tree,Ensemble Method,Input Feature,Learning Algorithm,Training Data}
}

@article{dorri2018multi,
  title = {Multi-Agent Systems: {{A}} Survey},
  author = {Dorri, Ali and Kanhere, Salil S and Jurdak, Raja},
  year = 2018,
  journal = {Ieee Access},
  volume = {6},
  pages = {28573--28593},
  publisher = {IEEE},
  keywords = {ToRead},
  file = {C:\Users\uif72473\Zotero\storage\3YHQ54XW\Dorri et al. - 2018 - Multi-agent systems A survey.pdf}
}

@article{doshi2017towards,
  title = {Towards a Rigorous Science of Interpretable Machine Learning},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = 2017,
  journal = {arXiv preprint arXiv:1702.08608},
  eprint = {1702.08608},
  archiveprefix = {arXiv}
}

@article{fourezEnsembleMultiAgentSystem2022,
  title = {An Ensemble {{Multi-Agent System}} for Non-Linear Classification},
  author = {Fourez, Thibault and Verstaevel, Nicolas and Migeon, Fr{\'e}d{\'e}ric and Schettini, Fr{\'e}d{\'e}ric and Amblard, Frederic},
  year = 2022,
  month = sep,
  journal = {arXiv},
  doi = {10.48550/arXiv.2209.06824},
  abstract = {Self-Adaptive Multi-Agent Systems (AMAS) transform machine learning problems into problems of local cooperation between agents. We present smapy, an ensemble based AMAS implementation for mobility prediction, whose agents are provided with machine learning models in addition to their cooperation rules. With a detailed methodology, we show that it is possible to use linear models for nonlinear classification on a benchmark transport mode detection dataset, if they are integrated in a cooperative multi-agent structure. The results obtained show a significant improvement of the performance of linear models in non-linear contexts thanks to the multi-agent approach.},
  langid = {english},
  keywords = {Artificial Intelligence (cs.AI),Machine Learning (cs.LG),Multiagent Systems (cs.MA)},
  file = {C:\Users\uif72473\Zotero\storage\U463RZQH\Fourez et al. - 2022 - An ensemble Multi-Agent System for non-linear clas.pdf}
}

@article{freund1997decision,
  title = {A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting},
  author = {Freund, Yoav and Schapire, Robert E},
  year = 1997,
  journal = {Journal of computer and system sciences},
  volume = {55},
  number = {1},
  pages = {119--139},
  publisher = {Elsevier}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy Function Approximation: {{A}} Gradient Boosting Machine.},
  author = {Friedman, Jerome H.},
  year = 2001,
  month = oct,
  journal = {Ann. Stat.},
  volume = {29},
  number = {5},
  pages = {1189--1232},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  doi = {10.1214/aos/1013203451},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent ``boosting'' paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such ``TreeBoost'' models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
  langid = {english},
  keywords = {62-02,62-07,62-08,62G08,62H30,68T10,boosting,decision trees,Function estimation,robust nonparametric regression}
}

@article{gijsberts2013real,
  title = {Real-Time Model Learning Using Incremental Sparse Spectrum Gaussian Process Regression},
  author = {Gijsberts, Arjan and Metta, Giorgio},
  year = 2013,
  journal = {Neural networks},
  volume = {41},
  pages = {59--69},
  publisher = {Elsevier}
}

@inproceedings{gilpin2018explaining,
  title = {Explaining Explanations: {{An}} Overview of Interpretability of Machine Learning},
  booktitle = {2018 {{IEEE}} 5th {{International Conference}} on Data Science and Advanced Analytics ({{DSAA}})},
  author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = 2018,
  pages = {80--89},
  publisher = {IEEE}
}

@article{gomez2023adaptive,
  title = {Adaptive Sparse {{Gaussian}} Process},
  author = {{Gomez-Verdejo}, Vanessa and {Parrado-Hernandez}, Emilio and {Martinez-Ramon}, Manel},
  year = 2023,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  publisher = {IEEE}
}

@article{haarnoja2018soft,
  title = {Soft Actor-Critic Algorithms and Applications},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  year = 2018,
  journal = {arXiv preprint arXiv:1812.05905},
  eprint = {1812.05905},
  archiveprefix = {arXiv}
}

@article{hullermeierAleatoricEpistemicUncertainty2021,
  title = {Aleatoric and Epistemic Uncertainty in Machine Learning: An Introduction to Concepts and Methods},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = 2021,
  month = mar,
  journal = {Mach. Learn.},
  volume = {110},
  number = {3},
  pages = {457--506},
  publisher = {Springer US},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-05946-3},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  keywords = {Artificial Intelligence,Control,Machine Learning,Mechatronics,Natural Language Processing (NLP),Robotics,Simulation and Modeling},
  file = {C:\Users\uif72473\Zotero\storage\CCDXJ2JJ\Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf}
}

@article{Jamil2013ALS,
  title = {A Literature Survey of Benchmark Functions for Global Optimisation Problems},
  author = {Jamil, Momin and Yang, Xin-She},
  year = 2013,
  journal = {ArXiv},
  volume = {abs/1308.4008}
}

@incollection{jolliffe2011principal,
  title = {Principal Component Analysis},
  booktitle = {International Encyclopedia of Statistical Science},
  author = {Jolliffe, Ian},
  year = 2011,
  pages = {1094--1096},
  publisher = {Springer}
}

@article{kalman1960contributions,
  title = {Contributions to the Theory of Optimal Control},
  author = {Kalman, Rudolf Emil and others},
  year = 1960,
  journal = {Bol. soc. mat. mexicana},
  volume = {5},
  number = {2},
  pages = {102--119}
}

@article{keLightGBMHighlyEfficient2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = 2017,
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  langid = {english}
}

@article{konstantinov2023interpretable,
  title = {Interpretable Ensembles of Hyper-Rectangles as Base Models},
  author = {Konstantinov, Andrei V and Utkin, Lev V},
  year = 2023,
  journal = {Neural Computing and Applications},
  volume = {35},
  number = {29},
  pages = {21771--21795},
  publisher = {Springer}
}

@inproceedings{le2017gogp,
  title = {Gogp: {{Fast}} Online Regression with Gaussian Processes},
  booktitle = {2017 {{IEEE}} International Conference on Data Mining ({{ICDM}})},
  author = {Le, Trung and Nguyen, Khanh and Nguyen, Vu and Nguyen, Tu Dinh and Phung, Dinh},
  year = 2017,
  pages = {257--266},
  publisher = {IEEE}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = 2015,
  month = may,
  journal = {Nature},
  volume = {521},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  keywords = {Computer science,Mathematics and computing}
}

@article{linardatosExplainableAIReview2020,
  title = {Explainable {{AI}}: {{A Review}} of {{Machine Learning Interpretability Methods}}},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = 2020,
  month = dec,
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners. Keywords: xai; machine learning; explainability; interpretability; fairness; sensitivity; black-box},
  langid = {english},
  keywords = {black-box,explainability,fairness,interpretability,machine learning,sensitivity,ToRead,xai},
  file = {C:\Users\uif72473\Zotero\storage\KGAXEQZK\Linardatos et al. - 2020 - Explainable AI A Review of Machine Learning Inter.pdf}
}

@article{lowe1988multivariable,
  title = {Multivariable Functional Interpolation and Adaptive Networks},
  author = {Lowe, David and Broomhead, D},
  year = 1988,
  journal = {Complex systems},
  volume = {2},
  number = {3},
  pages = {321--355}
}

@article{loyola-gonzalezBlackBoxVsWhiteBox2019,
  title = {Black-{{Box}} vs. {{White-Box}}: {{Understanding Their Advantages}} and {{Weaknesses From}} a {{Practical Point}} of {{View}}},
  author = {{Loyola-Gonz{\'a}lez}, Octavio},
  year = 2019,
  month = oct,
  journal = {IEEE Access},
  volume = {7},
  pages = {154096--154113},
  publisher = {IEEE},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2949286},
  abstract = {Nowadays, in the international scientific community of machine learning, there exists an enormous discussion about the use of black-box models or explainable models; especially in practical problems. On the one hand, a part of the community defends that black-box models are more accurate than explainable models in some contexts, like image preprocessing. On the other hand, there exist another part of the community alleging that explainable models are better than black-box models because they can obtain comparable results and also they can explain these results in a language close to a human expert by using patterns. In this paper, advantages and weaknesses for each approach are shown; taking into account a state-of-the-art review for both approaches, their practical applications, trends, and future challenges. This paper shows that both approaches are suitable for solving practical problems, but experts in machine learning need to understand the input data, the problem to solve, and the best way for showing the output data before applying a machine learning model. Also, we propose some ideas for fusing both, explainable and black-box, approaches to provide better solutions to experts in real-world domains. Additionally, we show one way to measure the effectiveness of the applied machine learning model by using expert opinions jointly with statistical methods. Throughout this paper, we show the impact of using explainable and black-box models on the security and medical applications.},
  langid = {english},
  keywords = {ToRead},
  file = {C:\Users\uif72473\Zotero\storage\B3UX3D2L\Loyola-González - 2019 - Black-Box vs. White-Box Understanding Their Advan.pdf}
}

@article{lundberg2017unified,
  title = {A Unified Approach to Interpreting Model Predictions},
  author = {Lundberg, Scott M and Lee, Su-In},
  year = 2017,
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@inproceedings{ma2017informative,
  title = {Informative Planning and Online Learning with Sparse Gaussian Processes},
  booktitle = {2017 {{IEEE}} International Conference on Robotics and Automation ({{ICRA}})},
  author = {Ma, Kai-Chieh and Liu, Lantao and Sukhatme, Gaurav S},
  year = 2017,
  pages = {4292--4298},
  publisher = {IEEE}
}

@article{maiworm2021online,
  title = {Online Learning-Based Model Predictive Control with {{Gaussian}} Process Models and Stability Guarantees},
  author = {Maiworm, Michael and Limon, Daniel and Findeisen, Rolf},
  year = 2021,
  journal = {International Journal of Robust and Nonlinear Control},
  volume = {31},
  number = {18},
  pages = {8785--8812},
  publisher = {Wiley Online Library}
}

@article{mcinnes2018umap,
  title = {Umap: {{Uniform}} Manifold Approximation and Projection for Dimension Reduction},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = 2018,
  journal = {arXiv preprint arXiv:1802.03426},
  eprint = {1802.03426},
  archiveprefix = {arXiv}
}

@article{meier2014incremental,
  title = {Incremental Local Gaussian Regression},
  author = {Meier, Franziska and Hennig, Philipp and Schaal, Stefan},
  year = 2014,
  journal = {Advances in Neural Information Processing Systems},
  volume = {27}
}

@article{naish2007generalized,
  title = {The Generalized {{FITC}} Approximation},
  author = {{Naish-Guzman}, Andrew and Holden, Sean},
  year = 2007,
  journal = {Advances in neural information processing systems},
  volume = {20}
}

@book{nesterov2013introductory,
  title = {Introductory Lectures on Convex Optimization: {{A}} Basic Course},
  author = {Nesterov, Yurii},
  year = 2013,
  volume = {87},
  publisher = {Springer Science \& Business Media}
}

@article{nguyen2009model,
  title = {Model Learning with Local Gaussian Process Regression},
  author = {{Nguyen-Tuong}, Duy and Seeger, Matthias and Peters, Jan},
  year = 2009,
  journal = {Advanced Robotics},
  volume = {23},
  number = {15},
  pages = {2015--2034},
  publisher = {Taylor \& Francis}
}

@article{pedregosaScikitlearnMachineLearning2012,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and M{\"u}ller, Andreas and Nothman, Joel and Louppe, Gilles and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  year = 2012,
  month = jan,
  journal = {arXiv},
  doi = {10.48550/arXiv.1201.0490},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from this http URL.},
  langid = {english},
  keywords = {Machine Learning (cs.LG),Mathematical Software (cs.MS)}
}

@incollection{polikarEnsembleLearning2012,
  title = {Ensemble {{Learning}}},
  booktitle = {{{SpringerLink}}},
  author = {Polikar, Robi},
  year = 2012,
  month = jan,
  pages = {1--34},
  publisher = {Springer, New York, NY},
  address = {New York, NY, USA},
  doi = {10.1007/978-1-4419-9326-7_1},
  abstract = {Over the last couple of decades, multiple classifier systems, also called ensemble systems have enjoyed growing attention within the computational intelligence and machine learning community. This attention has been well deserved, as ensemble systems have proven...},
  isbn = {978-1-4419-9326-7},
  langid = {english},
  keywords = {Combination Rule,Concept Drift,Ensemble Member,Incremental Learning,Majority Vote}
}

@article{prag2022toward,
  title = {Toward Data-Driven Optimal Control: {{A}} Systematic Review of the Landscape},
  author = {Prag, Krupa and Woolway, Matthew and Celik, Turgay},
  year = 2022,
  journal = {IEEE Access},
  volume = {10},
  pages = {32190--32212},
  publisher = {IEEE}
}

@article{rawlings2017model,
  title = {Model Predictive Control: Theory, Computation, and Design, Vol. 2},
  author = {Rawlings, James Blake and Mayne, David Q and Diehl, Moritz and others},
  year = 2017,
  journal = {Madison, WI: Nob Hill Publishing}
}

@inproceedings{ribeiro2016should,
  title = {" {{Why}} Should i Trust You?" {{Explaining}} the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = 2016,
  pages = {1135--1144}
}

@article{rohbogner2014design,
  title = {Design of a Multiagent-Based Voltage Control System in Peer-to-Peer Networks for Smart Grids},
  author = {Rohbogner, Gregor and Fey, Simon and Benoit, Pascal and Wittwer, Christof and Christ, Andreas},
  year = 2014,
  journal = {Energy Technology},
  volume = {2},
  number = {1},
  pages = {107--120},
  publisher = {Wiley Online Library}
}

@incollection{rokachDecisionTrees2005,
  title = {Decision {{Trees}}},
  booktitle = {{{SpringerLink}}},
  author = {Rokach, Lior and Maimon, Oded},
  year = 2005,
  pages = {165--192},
  publisher = {Springer, Boston, MA},
  address = {Boston, MA, USA},
  doi = {10.1007/0-387-25465-X_9},
  abstract = {Decision Trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and Data Mining have dealt with the issue of growing a decision tree...},
  isbn = {978-0-387-25465-4},
  langid = {english},
  keywords = {C4.5,CART,Decision tree,Gain Ratio,Gini Index,Information Gain,Length,Minimum Description,Oblivious Decision Trees,Pruning}
}

@article{schulzTutorialGaussianProcess2018,
  title = {A Tutorial on {{Gaussian}} Process Regression: {{Modelling}}, Exploring, and Exploiting Functions},
  author = {Schulz, Eric and Speekenbrink, Maarten and Krause, Andreas},
  year = 2018,
  month = aug,
  journal = {J. Math. Psychol.},
  volume = {85},
  pages = {1--16},
  publisher = {Academic Press},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2018.03.001},
  abstract = {This tutorial introduces the reader to Gaussian process regression as an expressive tool to model, actively explore and exploit unknown functions. Gaussian process regression is a powerful, non-parametric Bayesian approach towards regression problems that can be utilized in exploration and exploitation scenarios. This tutorial aims to provide an accessible introduction to these techniques. We will introduce Gaussian processes which generate distributions over functions used for Bayesian non-parametric regression, and demonstrate their use in applications and didactic examples including simple regression problems, a demonstration of kernel-encoded prior assumptions and compositions, a pure exploration scenario within an optimal design framework, and a bandit-like exploration--exploitation scenario where the goal is to recommend movies. Beyond that, we describe a situation modelling risk-averse exploration in which an additional constraint (not to sample below a certain threshold) needs to be accounted for. Lastly, we summarize recent psychological experiments utilizing Gaussian processes. Software and literature pointers are also provided.},
  langid = {english},
  keywords = {Active learning,Bandit problems,Exploration-exploitation,Gaussian process regression}
}

@article{seeger2004gaussian,
  title = {Gaussian Processes for Machine Learning},
  author = {Seeger, Matthias},
  year = 2004,
  journal = {International journal of neural systems},
  volume = {14},
  number = {02},
  pages = {69--106},
  publisher = {World Scientific}
}

@article{SHAMSHIRBAND20132105,
  title = {An Appraisal and Design of a Multi-Agent System Based Cooperative Wireless Intrusion Detection Computational Intelligence Technique},
  author = {Shamshirband, Shahaboddin and Anuar, Nor Badrul and Kiah, Miss Laiha Mat and Patel, Ahmed},
  year = 2013,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {26},
  number = {9},
  pages = {2105--2127},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2013.04.010},
  abstract = {The deployment of wireless sensor networks and mobile ad-hoc networks in applications such as emergency services, warfare and health monitoring poses the threat of various cyber hazards, intrusions and attacks as a consequence of these networks' openness. Among the most significant research difficulties in such networks safety is intrusion detection, whose target is to distinguish between misuse and abnormal behavior so as to ensure secure, reliable network operations and services. Intrusion detection is best delivered by multi-agent system technologies and advanced computing techniques. To date, diverse soft computing and machine learning techniques in terms of computational intelligence have been utilized to create Intrusion Detection and Prevention Systems (IDPS), yet the literature does not report any state-of-the-art reviews investigating the performance and consequences of such techniques solving wireless environment intrusion recognition issues as they gain entry into cloud computing. The principal contribution of this paper is a review and categorization of existing IDPS schemes in terms of traditional artificial computational intelligence with a multi-agent support. The significance of the techniques and methodologies and their performance and limitations are additionally analyzed in this study, and the limitations are addressed as challenges to obtain a set of requirements for IDPS in establishing a collaborative-based wireless IDPS (Co-WIDPS) architectural design. It amalgamates a fuzzy reinforcement learning knowledge management by creating a far superior technological platform that is far more accurate in detecting attacks. In conclusion, we elaborate on several key future research topics with the potential to accelerate the progress and deployment of computational intelligence based Co-WIDPSs.},
  keywords = {Cloud computing,Collaborative IDPS,Computational intelligence,Intrusion detection and prevention systems (IDPS),Multi-agent systems,Wireless sensor networks}
}

@article{snelson2005sparse,
  title = {Sparse {{Gaussian}} Processes Using Pseudo-Inputs},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = 2005,
  journal = {Advances in neural information processing systems},
  volume = {18}
}

@article{stable-baselines3,
  title = {Stable-Baselines3: {{Reliable}} Reinforcement Learning Implementations},
  author = {Raffin, Antonin and Hill, Ashley and Gleave, Adam and Kanervisto, Anssi and Ernestus, Maximilian and Dormann, Noah},
  year = 2021,
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {268},
  pages = {1--8}
}

@inproceedings{titsias2009variational,
  title = {Variational Learning of Inducing Variables in Sparse {{Gaussian}} Processes},
  booktitle = {Artificial Intelligence and Statistics},
  author = {Titsias, Michalis},
  year = 2009,
  pages = {567--574},
  publisher = {PMLR}
}

@inproceedings{todorov2012mujoco,
  title = {Mujoco: {{A}} Physics Engine for Model-Based Control},
  booktitle = {2012 {{IEEE}}/{{RSJ}} International Conference on Intelligent Robots and Systems},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  year = 2012,
  pages = {5026--5033},
  publisher = {IEEE}
}

@article{towers2024gymnasium,
  title = {Gymnasium: A Standard Interface for Reinforcement Learning Environments},
  author = {Towers, Mark and Kwiatkowski, Ariel and Terry, Jordan and Balis, John U and De Cola, Gianluca and Deleu, Tristan and Goul{\~a}o, Manuel and Kallinteris, Andreas and Krimmel, Markus and KG, Arjun and others},
  year = 2024,
  journal = {arXiv preprint arXiv:2407.17032},
  eprint = {2407.17032},
  archiveprefix = {arXiv}
}

@book{weisbergAppliedLinearRegression2005,
  title = {Applied {{Linear Regression}}},
  author = {Weisberg, Sanford},
  year = 2005,
  month = jan,
  journal = {Wiley Online Library},
  publisher = {John Wiley \& Sons, Ltd.},
  address = {Chichester, England, UK},
  doi = {10.1002/0471704091},
  abstract = {``\dots this is an excellent book which could easily be used as a course text\dots '' (International Statistical Institute, January 2006) "Twenty years after the release of the excellent previous edition, the author has succeeded in putting together a superb and inviting third edition\dots " (Technometrics, August 2005)},
  isbn = {978-0-471-66379-9},
  langid = {english}
}

@article{welford1962note,
  title = {Note on a Method for Calculating Corrected Sums of Squares and Products},
  author = {Welford, Barry Payne},
  year = 1962,
  journal = {Technometrics},
  volume = {4},
  number = {3},
  pages = {419--420},
  publisher = {Taylor \& Francis}
}

@article{williams1995gaussian,
  title = {Gaussian Processes for Regression},
  author = {Williams, Christopher and Rasmussen, Carl},
  year = 1995,
  journal = {Advances in neural information processing systems},
  volume = {8}
}

@article{wolpert1992stacked,
  title = {Stacked Generalization},
  author = {Wolpert, David H},
  year = 1992,
  journal = {Neural networks},
  volume = {5},
  number = {2},
  pages = {241--259},
  publisher = {Elsevier}
}

@article{yang2018online,
  title = {Online Sparse Multi-Output {{Gaussian}} Process Regression and Learning},
  author = {Yang, Le and Wang, Ke and Mihaylova, Lyudmila},
  year = 2018,
  journal = {IEEE Transactions on Signal and Information Processing over Networks},
  volume = {5},
  number = {2},
  pages = {258--272},
  publisher = {IEEE}
}

@article{yukselTwentyYearsMixture2012,
  title = {Twenty Years of Mixture of Experts},
  author = {Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  year = 2012,
  journal = {IEEE transactions on neural networks and learning systems},
  volume = {23},
  number = {8},
  pages = {1177--1193},
  publisher = {IEEE},
  isbn = {2162-237X}
}

@article{zhang2023sequential,
  title = {Sequential {{Gaussian}} Processes for Online Learning of Nonstationary Functions},
  author = {Zhang, Michael Minyi and Dumitrascu, Bianca and Williamson, Sinead A and Engelhardt, Barbara E},
  year = 2023,
  journal = {IEEE Transactions on Signal Processing},
  volume = {71},
  pages = {1539--1550},
  publisher = {IEEE}
}

@article{zhuangAdaBeliefOptimizerAdapting2020,
  title = {{{AdaBelief Optimizer}}: {{Adapting Stepsizes}} by the {{Belief}} in {{Observed Gradients}}},
  author = {Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James S.},
  year = 2020,
  month = oct,
  journal = {arXiv},
  doi = {10.48550/arXiv.2010.07468},
  abstract = {Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at this https URL},
  langid = {english},
  keywords = {Computer Vision and Pattern Recognition (cs.CV),Machine Learning (cs.LG),Machine Learning (stat.ML)},
  file = {C:\Users\uif72473\Zotero\storage\T6GL99CQ\Zhuang et al. - 2020 - AdaBelief Optimizer Adapting Stepsizes by the Bel.pdf}
}
